{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_prediccion.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VRmSUA5XQ7Sm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "# Modulo de optimización\n",
        "from scipy import optimize\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Marca ,Número de puertas, Número de asientos, Condición del vehículo,Transmisión,Tipo de Combustible,Tipo,\"Modelo (Ej: 2021, 2016)\",Procedencia,Kilometraje (recorrido en Km),Cilindrada (en cc),Estado de la pintura,¿La pintura está en buen estado?,\"¿El vehículo tuvo algún accidente? \",\"¿El vehículo cuenta con todos los accesorios? \",Costo en $us,"
      ],
      "metadata": {
        "id": "FRs4LuoXySqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = np.loadtxt('preparado.csv', delimiter=',', dtype=float)\n",
        "data_test = np.loadtxt('test.csv', delimiter=',', dtype=float)\n",
        "#print(data_train)\n",
        "#print(data_test)\n"
      ],
      "metadata": {
        "id": "MXB1YI3xy-KH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data_train[:,0:15]\n",
        "X_test = data_test[:,0:15]\n",
        "#print(X_test)"
      ],
      "metadata": {
        "id": "Z_cGbs2h3jyp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jk8BJ3BELSCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = data_train[:,15]\n",
        "y_test = data_test[:,15]\n",
        "#y_test.shape,y.shape , y_test\n",
        "m, m_test= y.size ,y_test.size\n",
        "m"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N98edbfZ1vRv",
        "outputId": "c92ef33f-ede8-4cb4-b5cc-58f79252c7ca"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "835"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
        "X_test = np.concatenate([np.ones((m_test, 1)), X_test], axis=1)\n"
      ],
      "metadata": {
        "id": "bgV38wZgLcGa"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_test"
      ],
      "metadata": {
        "id": "7KS7LaObL6hC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalEqn(X, y):\n",
        "    theta = np.zeros(X.shape[1])\n",
        "    theta = np.dot(np.dot(np.linalg.inv(np.dot(X.T,X)),X.T),y)\n",
        "    return theta"
      ],
      "metadata": {
        "id": "3nyxI22oKmmP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta = normalEqn(X, y);\n",
        "# Muestra los resultados optenidos a partir de la aplicación de la ecuación de la normal\n",
        "print('Theta calculado a partir de la ecuación de la normal: {:s}'.format(str(theta)));"
      ],
      "metadata": {
        "id": "16K1o3MGdAL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0466aed1-cdbb-4e70-ea11-fe0efccc3bdf"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theta calculado a partir de la ecuación de la normal: [-5.94619377e+05 -1.17266071e+02 -1.27651242e+01  6.21360133e+02\n",
            " -2.89827416e+03 -2.54590551e+03 -4.21942366e+03  3.56600789e+03\n",
            "  3.09723019e+02 -1.92247365e+02 -6.72445916e-04  6.07502094e-01\n",
            " -2.79657216e+02 -1.43083103e+03  1.02609883e+03 -1.30502577e+03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prices=[]\n",
        "for x_t in X_test:\n",
        "    prices.append(int(np.dot(x_t, theta)))\n",
        "print('precios Predecidos (usando la ecuación de la normal): $')\n",
        "print(prices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-kC3MHqNAoK",
        "outputId": "2f253a60-8cec-4453-cf87-f7ac82f84b9f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precios Predecidos (usando la ecuación de la normal): $\n",
            "[33338, 36980, 25582, 21062, 9355, 17446, 16678]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aVaG-wKNNbB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**usando una Red neuronal** "
      ],
      "metadata": {
        "id": "CcahTvQgbsJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#funciondes de activacion\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Computes the sigmoid of z.\n",
        "    \"\"\"\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "def reluPrime(x):\n",
        "  return x > 0"
      ],
      "metadata": {
        "id": "GjnTX8mo_l7X"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xGxyvFBeESxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.exp(x).sum(axis=-1,keepdims=True)"
      ],
      "metadata": {
        "id": "V_1-jsm8_m4H"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "funciones de perdida"
      ],
      "metadata": {
        "id": "J55vV7OcET20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Square Error -> usada para regresión (con activación lineal)\n",
        "def mse(y, y_hat):\n",
        "    return np.mean((y_hat - y.reshape(y_hat.shape))**2)\n",
        "\n",
        "# Binary Cross Entropy -> usada para clasificación binaria (con sigmoid)\n",
        "def bce(y, y_hat):\n",
        "    return - np.mean(y.reshape(y_hat.shape)*np.log(y_hat) - (1 - y.reshape(y_hat.shape))*np.log(1 - y_hat))\n",
        "\n",
        "# Cross Entropy (aplica softmax + cross entropy de manera estable) -> usada para clasificación multiclase\n",
        "def crossentropy(y, y_hat):\n",
        "    logits = y_hat[np.arange(len(y_hat)),y]\n",
        "    entropy = - logits + np.log(np.sum(np.exp(y_hat),axis=-1))\n",
        "    return entropy.mean()"
      ],
      "metadata": {
        "id": "oElbryEBBO-Z"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Derivadas"
      ],
      "metadata": {
        "id": "IZugCMy9EXSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_mse(y, y_hat):\n",
        "    return y_hat - y.reshape(y_hat.shape)\n",
        "\n",
        "def grad_bce(y, y_hat):\n",
        "    return y_hat - y.reshape(y_hat.shape)\n",
        "\n",
        "def grad_crossentropy(y, y_hat):\n",
        "    answers = np.zeros_like(y_hat)\n",
        "    answers[np.arange(len(y_hat)),y] = 1    \n",
        "    return (- answers + softmax(y_hat)) / y_hat.shape[0]"
      ],
      "metadata": {
        "id": "i9fIvOu3rn52"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T16:49:54.342062Z",
          "start_time": "2020-08-05T16:49:54.325063Z"
        },
        "code_folding": [
          3,
          14,
          20,
          55
        ],
        "id": "7O__X_wZA7qf"
      },
      "outputs": [],
      "source": [
        "# clase base MLP \n",
        "\n",
        "class MLP():\n",
        "  def __init__(self, D_in, H, D_out, loss, grad_loss, activation):\n",
        "    # pesos de la capa 1\n",
        "    self.w1, self.b1 = np.random.normal(loc=0.0,\n",
        "                                  scale=np.sqrt(2/(D_in+H)),\n",
        "                                  size=(D_in, H)), np.zeros(H)\n",
        "    # pesos de la capa 2\n",
        "    self.w2, self.b2 = np.random.normal(loc=0.0,\n",
        "                                  scale=np.sqrt(2/(H+D_out)),\n",
        "                                  size=(H, D_out)), np.zeros(D_out)\n",
        "    self.ws = []\n",
        "    # función de pérdida y derivada\n",
        "    self.loss = loss\n",
        "    self.grad_loss = grad_loss\n",
        "    # función de activación\n",
        "    self.activation = activation\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # salida de la capa 1\n",
        "    self.h_pre = np.dot(x, self.w1) + self.b1\n",
        "    self.h = relu(self.h_pre)\n",
        "    # salida del MLP\n",
        "    y_hat = np.dot(self.h, self.w2) + self.b2 \n",
        "    return self.activation(y_hat)\n",
        "    \n",
        "  def fit(self, X, Y, epochs = 100, lr = 0.001, batch_size=None, verbose=True, log_each=1):\n",
        "    batch_size = len(X) if batch_size == None else batch_size\n",
        "    batches = len(X) // batch_size\n",
        "    l = []\n",
        "    for e in range(1,epochs+1):     \n",
        "        # Mini-Batch Gradient Descent\n",
        "        _l = []\n",
        "        for b in range(batches):\n",
        "            # batch de datos\n",
        "            x = X[b*batch_size:(b+1)*batch_size]\n",
        "            y = Y[b*batch_size:(b+1)*batch_size] \n",
        "            # salida del perceptrón\n",
        "            y_pred = self(x) \n",
        "            # función de pérdida\n",
        "            loss = self.loss(y, y_pred)\n",
        "            _l.append(loss)        \n",
        "            # Backprop \n",
        "            dldy = self.grad_loss(y, y_pred) \n",
        "            grad_w2 = np.dot(self.h.T, dldy)\n",
        "            grad_b2 = dldy.mean(axis=0)\n",
        "            dldh = np.dot(dldy, self.w2.T)*reluPrime(self.h_pre)      \n",
        "            grad_w1 = np.dot(x.T, dldh)\n",
        "            grad_b1 = dldh.mean(axis=0)\n",
        "            # Update (GD)\n",
        "            self.w1 = self.w1 - lr * grad_w1\n",
        "            self.b1 = self.b1 - lr * grad_b1\n",
        "            self.w2 = self.w2 - lr * grad_w2\n",
        "            self.b2 = self.b2 - lr * grad_b2\n",
        "        l.append(np.mean(_l))\n",
        "        # guardamos pesos intermedios para visualización\n",
        "        self.ws.append((\n",
        "            self.w1.copy(),\n",
        "            self.b1.copy(),\n",
        "            self.w2.copy(),\n",
        "            self.b2.copy()\n",
        "        ))\n",
        "        if verbose and not e % log_each:\n",
        "            print(f'Epoch: {e}/{epochs}, Loss: {np.mean(l):.5f}')\n",
        "\n",
        "  def predict(self, ws, x):\n",
        "    w1, b1, w2, b2 = ws\n",
        "    h = relu(np.dot(x, w1) + b1)\n",
        "    y_hat = np.dot(h, w2) + b2\n",
        "    return self.activation(y_hat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP para regresión\n",
        "class MLPRegression(MLP):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super().__init__(D_in, H, D_out, mse, grad_mse, linear)\n",
        "\n",
        "# MLP para clasificación binaria\n",
        "class MLPBinaryClassification(MLP):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super().__init__(D_in, H, D_out, bce, grad_bce, sigmoid)\n",
        "\n",
        "# MLP para clasificación multiclase\n",
        "class MLPClassification(MLP):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super().__init__(D_in, H, D_out, crossentropy, grad_crossentropy, linear)"
      ],
      "metadata": {
        "id": "UjMkaUfLGiVO"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cargamos los datos de entrenamiento y de prueba\n"
      ],
      "metadata": {
        "id": "UcQolAykdfu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data_train[:,0:15]\n",
        "X_test = data_test[:,0:15]\n",
        "\n",
        "y = data_train[:,15]\n",
        "y_test = data_test[:,15]\n"
      ],
      "metadata": {
        "id": "ByQ2tBQJdfLF"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalización datos\n",
        "X_mean = X.mean(axis=0)\n",
        "X_std = X.std(axis=0)\n",
        "X_norm = (X - X_mean) / X_std"
      ],
      "metadata": {
        "id": "hw83l4cKUtet"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_norm.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzgC4DzEdU54",
        "outputId": "1237781f-448b-473b-b4a5-bc9e60fca0e4"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(835, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLPRegression(D_in=15, H=10, D_out=1)\n",
        "epochs, lr = 10, 0.0001\n",
        "model.fit(X_norm, y, epochs, lr, batch_size=1, log_each=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6hAfQmtc4y0",
        "outputId": "eef7a547-42a1-42c5-d014-330027dae814"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in multiply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10, Loss: nan\n",
            "Epoch: 2/10, Loss: nan\n",
            "Epoch: 3/10, Loss: nan\n",
            "Epoch: 4/10, Loss: nan\n",
            "Epoch: 5/10, Loss: nan\n",
            "Epoch: 6/10, Loss: nan\n",
            "Epoch: 7/10, Loss: nan\n",
            "Epoch: 8/10, Loss: nan\n",
            "Epoch: 9/10, Loss: nan\n",
            "Epoch: 10/10, Loss: nan\n"
          ]
        }
      ]
    }
  ]
}